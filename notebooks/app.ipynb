{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\10668186\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10668186\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask import Flask,render_template,url_for,request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Input, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional, Dense, \\\n",
    "    LSTM, Conv1D, Dropout, concatenate\n",
    "\n",
    "from preprocessor import clean_txt\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import constraints, initializers, regularizers\n",
    "from keras.engine import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                              K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    max_features = 15000\n",
    "    max_len = 150\n",
    "    embed_size = 150\n",
    "    \n",
    "    embedding_matrix = np.genfromtxt(\"glove_trained_embedding.csv\", delimiter=',')\n",
    "    main_input1 = Input(shape=(max_len,), name='main_input1')\n",
    "    x1 = (Embedding(max_features + 1, 300, input_length=max_len,\n",
    "                    weights=[embedding_matrix], trainable=False))(main_input1)\n",
    "    x1 = SpatialDropout1D(0.4)(x1)\n",
    "    x2 = Bidirectional(LSTM(75, dropout=0.5, return_sequences=True))(x1)\n",
    "    x = Dropout(0.55)(x2)\n",
    "    x = Bidirectional(LSTM(50, dropout=0.5, return_sequences=True))(x)\n",
    "    hidden = concatenate([\n",
    "        Attention(max_len)(x1),\n",
    "        Attention(max_len)(x2),\n",
    "        Attention(max_len)(x)\n",
    "    ])\n",
    "    hidden = Dense(32, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.5)(hidden)\n",
    "    hidden = Dense(16, activation='selu')(hidden)\n",
    "    hidden = Dropout(0.5)(hidden)\n",
    "    output_lay1 = Dense(8, activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=[main_input1], outputs=output_lay1)\n",
    "    model.load_weights(filepath='final_clf_model.hdf5')\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['binary_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('home.html')\n",
    "\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "\n",
    "def predict():\n",
    "    \n",
    "    if request.method == 'POST':\n",
    "        message = request.form['message']\n",
    "        data = [message]\n",
    "        \n",
    "        input_text = []\n",
    "        model = build_model()\n",
    "        clean_text =  clean_txt(data)\n",
    "        input_txt.append(clean_text)\n",
    "        max_features = 15000\n",
    "        max_len = 150\n",
    "\n",
    "\n",
    "        tk = Tokenizer(lower=True, filters='', num_words=max_features, oov_token=True)\n",
    "        tk.fit_on_texts(input_txt)\n",
    "        tokenized = tk.texts_to_sequences(input_txt)\n",
    "        x_test = pad_sequences(tokenized, maxlen=max_len)\n",
    "\n",
    "        vpp = model.predict(x_test)\n",
    "        vpp = vpp.flatten(order='C')\n",
    "        vpp_str = list(vpp)\n",
    "        vpp_str = ', '.join(map(str, vpp_str))\n",
    "        \n",
    "    return render_template('result.html', foobar = vpp_str)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    \n",
    "    input_text = []\n",
    "    model = build_model()\n",
    "    clean_text =  clean_txt(text)\n",
    "    input_txt.append(clean_text)\n",
    "    max_features = 15000\n",
    "    max_len = 150\n",
    "    \n",
    "    \n",
    "    tk = Tokenizer(lower=True, filters='', num_words=max_features, oov_token=True)\n",
    "    tk.fit_on_texts(input_txt)\n",
    "    tokenized = tk.texts_to_sequences(input_txt)\n",
    "    x_test = pad_sequences(tokenized, maxlen=max_len)\n",
    "    \n",
    "    vpp = model.predict(x_test)\n",
    "    vpp = vpp.flatten(order='C')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if america had another years of obama ideology via hillary we would be well on our way to being shithole country\n"
     ]
    }
   ],
   "source": [
    "text = \"if america had another years of obama ideology via hillary we would be well on our way to being shithole country\"\n",
    "\n",
    "clean_text =  clean_txt(text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = []\n",
    "input_txt.append(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 15000\n",
    "max_len = 150\n",
    "tk = Tokenizer(lower=True, filters='', num_words=max_features, oov_token=True)\n",
    "tk.fit_on_texts(input_txt)\n",
    "tokenized = tk.texts_to_sequences(input_txt)\n",
    "x_test = pad_sequences(tokenized, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpp = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3548488, 0.7206501, 0.10172686, 0.015522897, 0.03900522, 0.2573474, 0.020830274, 0.55477875'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpp = list(vpp)\n",
    "', '.join(map(str, vpp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpp = vpp.flatten(order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance: if america had another years of obama ideology via hillary we would be well on our way to being shithole country\n",
      "    Incites Violence: 0.3548488\n",
      "    No Violence: 0.6451511979103088\n",
      "  Scope:-\n",
      "    Direct: 0.7206501\n",
      "    Generalized: 0.2793499231338501\n",
      "  Hate Group:-\n",
      "    Gender: 0.10172686\n",
      "    Race: 0.015522897\n",
      "    Origin: 0.03900522\n",
      "    Disability: 0.2573474\n",
      "    Religion: 0.020830274\n",
      "    Sexual Orientation: 0.55477875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Instance:', input_txt[0])\n",
    "print('    Incites Violence:', vpp[0])\n",
    "print('    No Violence:', 1-vpp[0])\n",
    "print('  Scope:-')\n",
    "print('    Direct:', vpp[1])\n",
    "print('    Generalized:', 1-vpp[1])\n",
    "print('  Hate Group:-')\n",
    "print('    Gender:', vpp[2])\n",
    "print('    Race:', vpp[3])\n",
    "print('    Origin:', vpp[4])\n",
    "print('    Disability:', vpp[5])\n",
    "print('    Religion:', vpp[6])\n",
    "print('    Sexual Orientation:', vpp[7])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    \n",
    "    model = build_model()\n",
    "    model.load_weights(filepath='final_clf_model.hdf5')\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdeep",
   "language": "python",
   "name": "tfdeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
